#!/bin/bash
echo "//this code is generated by gen_tensor_dense_vml.sh
#pragma once

#include \"../common/monolish_common.hpp\"

namespace monolish {
/**
 * @brief
 * Vector and Matrix element-wise math library
 */
namespace vml {
"
echo "
/**
 * @addtogroup tensor_Dense_VML
 * @{
 */
"

## tensor_Dense tensor-tensor arithmetic
detail=(addition subtract multiplication division)
func=(add sub mul div)
for i in ${!detail[@]}; do
echo "
/**
 * \defgroup vml_dns${func[$i]} monolish::vml::${func[$i]}
 * @brief element by element ${detail[$i]} tensor_Dense tensor A and tensor_Dense tensor B.
 * @{
 */
/**
 * @brief element by element ${detail[$i]} tensor_Dense tensor A and tensor_Dense tensor B.
 * @param A monolish tensor_Dense tensor
 * @param B monolish tensor_Dense tensor
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
 *    - # of data transfer: 0
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      for arg3 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
        echo "void ${func[$i]}(const $arg1 &A, const $arg2 &B, $arg3 &C);"
      done
    done
  done
done
echo "/**@}*/"
done

echo ""
################################################################

## tensor_Dense tensor-scalar arithmetic
detail=(addition subtract multiplication division)
func=(add sub mul div)
for i in ${!detail[@]}; do
echo "
/**
 * \defgroup vml_sdns${func[$i]} monolish::vml::${func[$i]}
 * @brief element by element ${detail[$i]} scalar alpha and tensor_Dense tensor A.
 * @{
 */
/**
 * @brief element by element ${detail[$i]} scalar alpha and tensor_Dense tensor A.
 * @param A monolish tensor_Dense tensor
 * @param alpha scalar value
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
 *    - # of data transfer: 0
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      echo "void ${func[$i]}(const $arg1 &A, const $prec alpha, $arg2 &C);"
    done
  done
done
echo "/**@}*/"
done

echo ""
#############################################

## tensor-tensor pow
echo "
/**
 * \defgroup vml_dnspow monolish::vml::pow
 *@brief power to tensor_Dense tensor elements (C[0:N] = pow(A[0:N], B[0:N]))
 * @{
 */
/**
 *@brief power to tensor_Dense tensor elements (C[0:N] = pow(A[0:N], B[0:N]))
 * @param A monolish tensor_Dense tensor
 * @param B monolish tensor_Dense tensor
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      for arg3 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
        echo "void pow(const $arg1 &A, const $arg2 &B, $arg3 &C);"
      done
    done
  done
done
echo "/**@}*/"
 
echo "
/**
 * \defgroup vml_sdnspow monolish::vml::pow
 * @brief power to tensor_Dense tensor elements by scalar value (C[0:N] = pow(A[0:N], alpha))
 * @{
 */
/**
 * @brief power to tensor_Dense tensor elements by scalar value (C[0:N] = pow(A[0:N], alpha))
 * @param A monolish tensor_Dense tensor
 * @param alpha scalar value
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      echo "void pow(const $arg1 &A, const $prec alpha, $arg2 &C);"
    done
  done
done
echo "/**@}*/"

echo ""
#############################################
## 2arg math
math=(sin sqrt sinh asin asinh tan tanh atan atanh ceil floor sign exp)
for math in ${math[@]}; do
echo "
/**
 * \defgroup vml_dns$math monolish::vml::$math
 * @brief $math to tensor_Dense tensor elements (C[0:nnz] = $math(A[0:nnz]))
 * @{
 */
/**
 * @brief $math to tensor_Dense tensor elements (C[0:nnz] = $math(A[0:nnz]))
 * @param A monolish tensor_Dense tensor
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      echo "void $math(const $arg1 &A, $arg2 &C);"
    done
  done
done
echo "/**@}*/"
done

echo ""
#############################################

## tensor-tensor max min
detail=(greatest smallest)
func=(max min)
for i in ${!detail[@]}; do
echo "
/**
* \defgroup vml_dnsdns${func[$i]} monolish::vml::${func[$i]}
 * @brief Create a new tensor_Dense tensor with ${detail[$i]} elements of two tensors (C[0:nnz] = ${func[$i]}(A[0:nnz], B[0:nnz]))
 * @{
 */
/**
 * @brief Create a new tensor_Dense tensor with ${detail[$i]} elements of two tensors (C[0:nnz] = ${func[$i]}(A[0:nnz], B[0:nnz]))
 * @param A monolish tensor_Dense tensor
 * @param B monolish tensor_Dense tensor
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
 *    - # of data transfer: 0
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      for arg3 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
        echo "void ${func[$i]}(const $arg1 &A, const $arg2 &B, $arg3 &C);"
      done
    done
  done
done
echo "/**@}*/"
done

echo ""

## tensor-scalar max min
detail=(greatest smallest)
func=(max min)
for i in ${!detail[@]}; do
echo "
/**
* \defgroup vml_sdns${func[$i]} monolish::vml::${func[$i]}
 * @brief Create a new tensor_Dense tensor with ${detail[$i]} elements of tensor_Dense tensor or scalar (C[0:nnz] = ${func[$i]}(A[0:nnz], alpha))
 * @{
 */
/**
 * @brief Create a new tensor_Dense tensor with ${detail[$i]} elements of tensor_Dense tensor or scalar (C[0:nnz] = ${func[$i]}(A[0:nnz], alpha))
 * @param A monolish tensor_Dense tensor
 * @param alpha scalar value
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
 *    - # of data transfer: 0
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      echo "void ${func[$i]}(const $arg1 &A, const $prec alpha, $arg2 &C);"
    done
  done
done
echo "/**@}*/"
done

echo ""

## tensor_Dense tensor max min
detail=(greatest smallest)
func=(max min)
for i in ${!detail[@]}; do
echo "
/**
* \defgroup vml_dns${func[$i]} monolish::vml::${func[$i]}
 * @brief Finds the ${detail[$i]} element in tensor_Dense tensor (${func[$i]}(C[0:nnz]))
 * @{
 */
/**
 * @brief Finds the ${detail[$i]} element in tensor_Dense tensor (${func[$i]}(C[0:nnz]))
 * @param C monolish tensor_Dense tensor
 * @return ${detail[$i]} value
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    echo "[[nodiscard]] $prec ${func[$i]}(const $arg1 &C);"
  done
done
echo "/**@}*/"
done

echo ""
#############################################

## tensor_Dense tensor alo
echo "
/**
* \defgroup vml_sdnsalo monolish::vml::alo
 * @brief Asymmetric linear operation to tensor_Dense tensor elements (C[0:nnz] = alpha max(A[0:nnz], 0) + beta min(A[0:nnz], 0))
 * @{
 */
/**
 * @brief Asymmetric linear operation to tensor_Dense tensor elements (C[0:nnz] = alpha max(A[0:nnz], 0) + beta min(A[0:nnz], 0))
 * @param A monolish tensor_Dense tensor
 * @param alpha linear coefficient in positive range
 * @param beta linear coefficient in negative range
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      echo "void alo(const $arg1 &A, const $prec alpha, const $prec beta, $arg2 &C);"
    done
  done
done
echo "/**@}*/"

echo ""
#############################################

## reciprocal
echo "
/**
* \defgroup vml_dnsreciprocal monolish::vml::reciprocal
 * @brief Compute reciprocal to tensor_Dense tensor elements (C[0:nnz] = 1 / A[0:nnz])
 * @{
 */
/**
 * @brief Compute reciprocal to tensor_Dense tensor elements (C[0:nnz] = 1 / A[0:nnz])
 * @param A monolish tensor_Dense tensor
 * @param C monolish tensor_Dense tensor
 * @note
 * - # of computation: size
 * - Multi-threading: true
 * - GPU acceleration: true
*/ "
for prec in double float; do
  for arg1 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
    for arg2 in tensor::tensor_Dense\<$prec\> view_tensor_Dense\<vector\<$prec\>,$prec\> view_tensor_Dense\<matrix::Dense\<$prec\>,$prec\> view_tensor_Dense\<tensor::tensor_Dense\<$prec\>,$prec\>; do
      echo "void reciprocal(const $arg1 &A, $arg2 &C);"
    done
  done
done

echo "/**@}*/"
echo "/**@}*/"
echo "}"
echo "}"
